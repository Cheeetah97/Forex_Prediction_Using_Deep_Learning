{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pickle\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from Functions import *\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "data_path = \"M:/Dissertation/Data/\"\n",
    "results_path = \"M:/Dissertation/Return_Prediction/Deep_Learning/Results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Data\n",
    "orig_data = pd.read_csv(data_path+\"Forex_Data.csv\")\n",
    "orig_data[\"Date\"] = pd.to_datetime(orig_data[\"Date\"],format=\"%Y-%m-%d %H:00:00\")\n",
    "orig_data = orig_data.loc[(orig_data.Date>='2016-01-01')&(orig_data.Date<'2018-01-01')].reset_index(drop=True)\n",
    "\n",
    "for col in ['EUR/USD_T','EUR/GBP_T','GBP/USD_T','XAU/USD_T']:\n",
    "    orig_data[col] = orig_data[col.split('_')[0]+'_R']\n",
    "\n",
    "for col in ['EUR/USD_R','EUR/GBP_R','GBP/USD_R','XAU/USD_R']:\n",
    "    orig_data[col] = orig_data[col].shift(1)\n",
    "    \n",
    "orig_data = orig_data.dropna(subset=['EUR/USD_R','EUR/GBP_R','GBP/USD_R','XAU/USD_R'])\n",
    "orig_data = orig_data.sort_values(by=[\"Date\"]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HyperParameter Tuning using Time-Series CrossValidation\n",
    "eval_window_sizes = range(6,1,-1)\n",
    "eval_n_neurons = [4,6,8,10,12]\n",
    "eval_n_layers = [2,3,4,5,6]\n",
    "eval_learning_rate = [0.001,0.0025,0.005]\n",
    "eval_batch_size = [12,24,48,72]\n",
    "\n",
    "total_iterations = len(eval_window_sizes)*len(eval_n_neurons)*len(eval_n_layers)*len(eval_learning_rate)*len(eval_batch_size)\n",
    "counter = 1\n",
    "\n",
    "for window_size in eval_window_sizes:\n",
    "    for n_neurons in eval_n_neurons:\n",
    "        for n_layers in eval_n_layers:\n",
    "            for learning_rate in eval_learning_rate:\n",
    "                for batch_size in eval_batch_size:\n",
    "                    print(f\"##### Iteration: {counter} of {total_iterations} #####\")\n",
    "                    counter+=1\n",
    "\n",
    "                    data = orig_data.copy()\n",
    "\n",
    "                    # RNN Data Prep\n",
    "                    FEATURES = list(data.drop(['Date','EUR/USD_T','EUR/GBP_T','GBP/USD_T','XAU/USD_T'],axis=1).columns)\n",
    "                    TARGETS = ['EUR/USD_T','EUR/GBP_T','GBP/USD_T','XAU/USD_T']\n",
    "                    x,y,data = rnn_data_prep(data[FEATURES],data[TARGETS],window_size,data)\n",
    "\n",
    "                    # Extracting Test Sets for Evaluation\n",
    "                    test_portions = []\n",
    "                    test_portions_x = []\n",
    "                    test_portions_y = []\n",
    "                    TEST_SIZE = 71\n",
    "                    TEST_PORTIONS = 5\n",
    "                        \n",
    "                    for portion in generate_test_portions(data,TEST_SIZE,TEST_PORTIONS):\n",
    "                        test_portions.append(data.loc[portion,:].reset_index(drop=True))\n",
    "                        test_portions_x.append(x[portion,:])\n",
    "                        test_portions_y.append(y[portion,:])\n",
    "                        \n",
    "                        data = data.loc[~(data.index.isin(portion)),:]\n",
    "                        x = np.delete(x,portion,axis=0)\n",
    "                        y = np.delete(y,portion,axis=0)\n",
    "                    train_data = data.reset_index(drop=True).copy()\n",
    "\n",
    "\n",
    "                    # Getting Train and Validation Sets for Training\n",
    "                    FOLDS = 5\n",
    "                    SELECTED_FOLDS = 5\n",
    "                    train_portions_x,train_portions_y,valid_portions_x,valid_portions_y = get_folds(x,y,train_data,FOLDS,SELECTED_FOLDS)\n",
    "                    \n",
    "                    valid_mses = []\n",
    "                    for fold in range(SELECTED_FOLDS):\n",
    "                        # Scale Features\n",
    "                        x_train = train_portions_x[fold]\n",
    "                        x_valid = valid_portions_x[fold]\n",
    "                        x_test = test_portions_x\n",
    "                        x_train_scaled,x_valid_scaled,x_test_scaled = x_scaler(x_train,x_valid,x_test,TSScaler())\n",
    "\n",
    "                        # Scale Targets\n",
    "                        y_train = train_portions_y[fold]\n",
    "                        y_valid = valid_portions_y[fold]\n",
    "                        y_test = test_portions_y\n",
    "                        y_train_scaled,y_valid_scaled,y_test_scaled = y_scaler(y_train,y_valid,y_test,TSScaler(range=(-1,1)))\n",
    "                    \n",
    "                        # Training the Model\n",
    "                        model = RNN(window_size,x_train_scaled.shape[2],n_neurons,n_layers,'tanh','mse',tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "                        model.fit([x_train_scaled],[y_train_scaled],verbose=False,use_multiprocessing=True,batch_size=batch_size,epochs=3,shuffle=True)\n",
    "\n",
    "                            # Loading Scaler Objects\n",
    "                        with open('scaler_y.pkl','rb') as file:\n",
    "                            y_scaler_obj = pickle.load(file)\n",
    "\n",
    "                        # Predicting the Validation Set\n",
    "                        valid_pred = model.predict([x_valid_scaled],verbose=False)\n",
    "                        valid_pred = y_scaler_obj.inverse_transform(valid_pred)\n",
    "                        valid_mse = 0\n",
    "                        for i in range(valid_pred.shape[1]):\n",
    "                            valid_mse += mean_squared_error(y_valid[:,i],valid_pred[:,i])\n",
    "                        valid_mses.append(valid_mse)\n",
    "                    tf.keras.backend.clear_session()\n",
    "\n",
    "                    results = {\"Total_Error\":[np.mean(valid_mses)],\"Features\":\"CurrentReturns+CurrentPrices\",\n",
    "                                \"Attention\":\"Removed\",\"LR,Layers,Neurons,BatchSize\":str((learning_rate,n_layers,n_neurons,batch_size)),\n",
    "                                \"Window\":[window_size],\"Model\":\"LSTM\"}\n",
    "                    results = pd.DataFrame(results)\n",
    "\n",
    "                    # Saving Results\n",
    "                    if os.path.isfile(results_path+'result_metrics.csv'):\n",
    "                        results.to_csv(results_path+'result_metrics.csv', mode='a', header=False, index=False)\n",
    "                    else:\n",
    "                        results.to_csv(results_path+'result_metrics.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
