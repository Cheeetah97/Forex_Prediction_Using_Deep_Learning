{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import pickle\n",
    "import tensorflow_probability as tfp\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from Functions import *\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "data_path = \"M:/Dissertation/Data/\"\n",
    "results_path = \"M:/Dissertation/Return_Prediction/Deep_Learning/Results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN Parameters\n",
    "window_size = 1\n",
    "learning_rate = 0.001\n",
    "n_neurons = 12\n",
    "n_layers = 2\n",
    "batch_size = 12\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Data\n",
    "data = pd.read_csv(data_path+\"Forex_Data.csv\")\n",
    "data[\"Date\"] = pd.to_datetime(data[\"Date\"],format=\"%Y-%m-%d %H:00:00\")\n",
    "data = data.loc[(data.Date>='2016-01-01')&(data.Date<'2018-01-01')].reset_index(drop=True)\n",
    "\n",
    "for col in ['EUR/USD_T','EUR/GBP_T','GBP/USD_T','XAU/USD_T']:\n",
    "    data[col] = data[col.split('_')[0]+'_R']\n",
    "\n",
    "for col in ['EUR/USD_R','EUR/GBP_R','GBP/USD_R','XAU/USD_R']:\n",
    "    data[col] = data[col].shift(1)\n",
    "    \n",
    "data = data.dropna(subset=['EUR/USD_R','EUR/GBP_R','GBP/USD_R','XAU/USD_R'])\n",
    "data = data.sort_values(by=[\"Date\"]).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape is:  (11678, 1, 8)\n",
      "Y Shape is:  (11678, 4)\n",
      "Data Shape is:  (11678, 13)\n"
     ]
    }
   ],
   "source": [
    "# RNN Data Prep\n",
    "FEATURES = list(data.drop(['Date','EUR/USD_T','EUR/GBP_T','GBP/USD_T','XAU/USD_T'],axis=1).columns)\n",
    "TARGETS = ['EUR/USD_T','EUR/GBP_T','GBP/USD_T','XAU/USD_T']\n",
    "\n",
    "x,y,data = rnn_data_prep(data[FEATURES],data[TARGETS],window_size,data)\n",
    "print(\"X Shape is: \",x.shape)\n",
    "print(\"Y Shape is: \",y.shape)\n",
    "print(\"Data Shape is: \",data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Shape is:  (11318, 1, 8)\n",
      "Y Shape is:  (11318, 4)\n",
      "Data Shape is:  (11318, 13)\n"
     ]
    }
   ],
   "source": [
    "# Extracting Test Sets for Evaluation\n",
    "test_portions = []\n",
    "test_portions_x = []\n",
    "test_portions_y = []\n",
    "TEST_SIZE = 71\n",
    "TEST_PORTIONS = 5\n",
    "\n",
    "for portion in generate_test_portions(data,TEST_SIZE,TEST_PORTIONS):\n",
    "    test_portions.append(data.loc[portion,:].reset_index(drop=True))\n",
    "    test_portions_x.append(x[portion,:])\n",
    "    test_portions_y.append(y[portion,:])\n",
    "    \n",
    "    data = data.loc[~(data.index.isin(portion)),:]\n",
    "    x = np.delete(x,portion,axis=0)\n",
    "    y = np.delete(y,portion,axis=0)\n",
    "    \n",
    "train_data = data.reset_index(drop=True).copy()\n",
    "print(\"X Shape is: \",x.shape)\n",
    "print(\"Y Shape is: \",y.shape)\n",
    "print(\"Data Shape is: \",train_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 1, 8)]            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 1, 12)             1008      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1, 12)             0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 12)                1200      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4)                 52        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,260\n",
      "Trainable params: 2,260\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Defining the Model\n",
    "model = RNN(window_size,x.shape[2],n_neurons,n_layers,'tanh','mse',tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "plot_model(model,to_file=results_path+'rnn_plot.png',show_shapes=True,show_layer_names=True)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Fold 1\n",
      "### Train size: 9054 Valid size: 2264 Test size: 360\n",
      "### Validation Loss: 3.17849483948998e-06\n",
      "#########################\n",
      "#########################\n",
      "### Fold 2\n",
      "### Train size: 9054 Valid size: 2264 Test size: 360\n",
      "### Validation Loss: 3.2320212781657987e-06\n",
      "#########################\n",
      "#########################\n",
      "### Fold 3\n",
      "### Train size: 9054 Valid size: 2264 Test size: 360\n",
      "### Validation Loss: 2.9379789019672656e-06\n",
      "#########################\n",
      "#########################\n",
      "### Fold 4\n",
      "### Train size: 9055 Valid size: 2263 Test size: 360\n",
      "### Validation Loss: 3.177204961766859e-06\n",
      "#########################\n",
      "#########################\n",
      "### Fold 5\n",
      "### Train size: 9055 Valid size: 2263 Test size: 360\n",
      "### Validation Loss: 3.2537450930242318e-06\n",
      "#########################\n",
      "\n",
      "\n",
      "#########################\n",
      "### Avg Validation Loss: 3.1558890148828276e-06\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "# Getting Train and Validation Sets for Training\n",
    "FOLDS = 5\n",
    "SELECTED_FOLDS = 5\n",
    "\n",
    "train_portions_x,train_portions_y,valid_portions_x,valid_portions_y = get_folds(x,y,train_data,FOLDS,SELECTED_FOLDS)\n",
    "\n",
    "valid_mses = []\n",
    "test_preds_all = pd.DataFrame()\n",
    "for fold in range(SELECTED_FOLDS):\n",
    "\n",
    "    # Scale Features\n",
    "    x_train = train_portions_x[fold]\n",
    "    x_valid = valid_portions_x[fold]\n",
    "    x_test = test_portions_x\n",
    "    x_train_scaled,x_valid_scaled,x_test_scaled = x_scaler(x_train,x_valid,x_test,TSScaler())\n",
    "\n",
    "    # Scale Targets\n",
    "    y_train = train_portions_y[fold]\n",
    "    y_valid = valid_portions_y[fold]\n",
    "    y_test = test_portions_y\n",
    "    y_train_scaled,y_valid_scaled,y_test_scaled = y_scaler(y_train,y_valid,y_test,TSScaler(range=(-1,1)))\n",
    "    \n",
    "    # Training the Model\n",
    "    model = RNN(window_size,x.shape[2],n_neurons,n_layers,'tanh','mse',tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "    model.fit([x_train_scaled],[y_train_scaled],validation_data=([x_valid_scaled],[y_valid_scaled]),batch_size=batch_size,use_multiprocessing=True,verbose=False,epochs=epochs,shuffle=True)\n",
    "\n",
    "    # Loading Scaler Objects\n",
    "    with open('scaler_y.pkl','rb') as file:\n",
    "        y_scaler_obj = pickle.load(file)\n",
    "\n",
    "    # Predicting the Validation Set\n",
    "    valid_pred = model.predict([x_valid_scaled],verbose=False)\n",
    "    valid_pred = y_scaler_obj.inverse_transform(valid_pred)\n",
    "    valid_mse = 0\n",
    "    for i in range(valid_pred.shape[1]):\n",
    "        valid_mse += mean_squared_error(y_valid[:,i],valid_pred[:,i])\n",
    "    valid_mses.append(valid_mse)\n",
    "\n",
    "    # Predicting the Test Set\n",
    "    test_pred_df = pd.DataFrame()\n",
    "    test_portions_copy = test_portions.copy()\n",
    "    for i in range(TEST_PORTIONS):\n",
    "        test_pred = model.predict([x_test_scaled[i]],verbose=False)\n",
    "        test_pred = y_scaler_obj.inverse_transform(test_pred)\n",
    "        test_pred = pd.DataFrame(test_pred,columns=['EUR/USD_P','EUR/GBP_P','GBP/USD_P','XAU/USD_P'])\n",
    "        test_portions_copy[i] = pd.concat([test_portions_copy[i],test_pred],axis=1)\n",
    "\n",
    "        # Saving Predictions\n",
    "        for col in [\"GBP/USD\",\"EUR/USD\",\"EUR/GBP\",\"XAU/USD\"]:\n",
    "            test_portions_copy[i][col+'_PP'] = (test_portions_copy[i][col+'_P']+1) * test_portions_copy[i][col]\n",
    "            test_portions_copy[i][col+'_PP'] = test_portions_copy[i][col+'_PP'].shift(1)\n",
    "\n",
    "        test_portions_copy[i][\"Portion\"] = i\n",
    "        test_pred_df = pd.concat([test_pred_df,test_portions_copy[i][['Date','Portion']+TARGETS+['EUR/USD_P','EUR/GBP_P','GBP/USD_P','XAU/USD_P']+['EUR/USD_PP','EUR/GBP_PP','GBP/USD_PP','XAU/USD_PP']+['EUR/USD','EUR/GBP','GBP/USD','XAU/USD']]])\n",
    "    test_preds_all = pd.concat([test_preds_all,test_pred_df])\n",
    "\n",
    "    print('#' * 25)\n",
    "    print('### Fold', fold + 1)\n",
    "    print('### Train size:', len(x_train_scaled), 'Valid size:', len(x_valid_scaled), 'Test size:', len(x_test_scaled[0])*TEST_PORTIONS)\n",
    "    print('### Validation Loss:', valid_mse)\n",
    "    print('#' * 25)\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "print(\"\\n\")\n",
    "print('#' * 25)\n",
    "print('### Avg Validation Loss:', np.mean(valid_mses))\n",
    "print('#' * 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.421605989423089e-06"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Averaging the Predictions of all Folds\n",
    "test_preds_all = test_preds_all.groupby(by=[\"Date\",\"Portion\"],as_index=False).mean()\n",
    "test_preds_all.to_csv(results_path+\"Test_Results.csv\",index=False)\n",
    "\n",
    "# Calculating MSE for each Portion\n",
    "res = {}\n",
    "results = pd.DataFrame()\n",
    "for i in range(TEST_PORTIONS):\n",
    "    for col in [\"GBP/USD\",\"EUR/USD\",\"EUR/GBP\",\"XAU/USD\"]:\n",
    "        res[col] = [mean_squared_error(test_preds_all.loc[test_preds_all.Portion==i,col+'_P'],test_preds_all.loc[test_preds_all.Portion==i,col+'_T'])]\n",
    "    results = pd.concat([results,pd.DataFrame(res)])\n",
    "\n",
    "# Average Error of Test Portions\n",
    "results = results.reset_index(drop=True)\n",
    "results = pd.DataFrame(results.mean()).transpose().reset_index(drop=True)\n",
    "results.loc[0].sum()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
