{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Import Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "from accelerate import Accelerator\n",
    "from torch.optim import AdamW\n",
    "from evaluate import load\n",
    "from typing import Optional, Iterable\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from transformers import PretrainedConfig\n",
    "from transformers import InformerConfig, InformerForPrediction\n",
    "from gluonts.dataset.common import ListDataset\n",
    "from gluonts.itertools import Cached, Cyclic\n",
    "from gluonts.dataset.loader import as_stacked_batches\n",
    "from gluonts.transform.sampler import InstanceSampler\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.multivariate_grouper import MultivariateGrouper\n",
    "from gluonts.time_feature import get_lags_for_frequency\n",
    "from gluonts.time_feature import time_features_from_frequency_str\n",
    "from gluonts.time_feature import TimeFeature\n",
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.transform import (AddAgeFeature,AddObservedValuesIndicator,AddTimeFeatures,AsNumpyArray,Chain,ExpectedNumInstanceSampler,\n",
    "                               InstanceSplitter,RemoveFields,SelectFields,SetField,TestSplitSampler,Transformation,ValidationSplitSampler,VstackFeatures,RenameFields)\n",
    "from Functions import *\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "data_path = \"M:/Dissertation/Data/\"\n",
    "test_data_path = \"M:/Dissertation/Return_Prediction/Deep_Learning/Results/\"\n",
    "results_path = \"M:/Dissertation/Return_Prediction/Transformer/Results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Set Informer Params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Related\n",
    "freq = '1H'                            # Freq of Data Samples\n",
    "prediction_length = 1                  # Prediction Horizon\n",
    "context_length = prediction_length * 7 # Look Back\n",
    "num_of_variates = 4                    # Number of Time Series (Target Variables)\n",
    "num_time_features = 4                  # Number of Time Features during Transformation\n",
    "lags = [1]                             # Number of Lags to Consider\n",
    "\n",
    "# Model Related\n",
    "learning_rate = 6e-4\n",
    "epochs = 25\n",
    "dropout = 0.3\n",
    "encoder_layers = 6\n",
    "decoder_layers = 4\n",
    "d_model = 32\n",
    "attention_type = 'full'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extracting Test Portions**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Portion</th>\n",
       "      <th>MinDate</th>\n",
       "      <th>MaxDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-20 23:00:00</td>\n",
       "      <td>2016-06-24 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-06 08:00:00</td>\n",
       "      <td>2017-11-09 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-09-07 10:00:00</td>\n",
       "      <td>2017-09-12 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2016-07-22 08:00:00</td>\n",
       "      <td>2016-07-27 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2016-10-31 06:00:00</td>\n",
       "      <td>2016-11-03 08:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Portion             MinDate             MaxDate\n",
       "0        0 2016-06-20 23:00:00 2016-06-24 01:00:00\n",
       "1        1 2017-11-06 08:00:00 2017-11-09 10:00:00\n",
       "2        2 2017-09-07 10:00:00 2017-09-12 13:00:00\n",
       "3        3 2016-07-22 08:00:00 2016-07-27 11:00:00\n",
       "4        4 2016-10-31 06:00:00 2016-11-03 08:00:00"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_portions = pd.read_csv(test_data_path+\"Test_Results.csv\")\n",
    "test_portions[\"Date\"] = pd.to_datetime(test_portions[\"Date\"],format=\"%Y-%m-%d %H:00:00\")\n",
    "test_portions = pd.concat([test_portions.groupby(by=[\"Portion\"],as_index=False).agg({\"Date\":\"min\"}),\n",
    "                           test_portions.groupby(by=[\"Portion\"]).agg({\"Date\":\"max\"})],ignore_index=True,axis=1).rename(columns={0:\"Portion\",1:\"MinDate\",2:\"MaxDate\"})\n",
    "test_portions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the Model and Generating Predictions for Each Test Portion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Test Portion 1\n",
      "### Average Valid MSE =  9.631020622341163e-07\n",
      "### Average Test MSE =  1.6203801900123401e-06\n",
      "#########################\n",
      "#########################\n",
      "### Test Portion 2\n",
      "### Average Valid MSE =  6.148287286297474e-07\n",
      "### Average Test MSE =  4.738443974832554e-07\n",
      "#########################\n",
      "#########################\n",
      "### Test Portion 3\n",
      "### Average Valid MSE =  5.965429248337623e-07\n",
      "### Average Test MSE =  7.958484591976849e-07\n",
      "#########################\n",
      "#########################\n",
      "### Test Portion 4\n",
      "### Average Valid MSE =  1.49343179248529e-06\n",
      "### Average Test MSE =  7.94709269580877e-07\n",
      "#########################\n",
      "#########################\n",
      "### Test Portion 5\n",
      "### Average Valid MSE =  7.779795173200022e-07\n",
      "### Average Test MSE =  7.509038245457935e-07\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "# Reading the Data\n",
    "all_data = pd.read_csv(data_path+\"Forex_Data.csv\")\n",
    "all_data[\"Date\"] = pd.to_datetime(all_data[\"Date\"],format=\"%Y-%m-%d %H:00:00\")\n",
    "\n",
    "# Iterating over the Test Portions\n",
    "final_results = pd.DataFrame()\n",
    "for p in list(test_portions.Portion.unique()):\n",
    "    \n",
    "    # Slicing the Data as per the Min and Max Dates of Each Test Portion\n",
    "    min_date = test_portions.loc[test_portions.Portion==p,\"MinDate\"].iloc[0]\n",
    "    max_date = test_portions.loc[test_portions.Portion==p,\"MaxDate\"].iloc[0]\n",
    "    data = all_data.loc[(all_data.Date>=min_date-pd.DateOffset(years=3))&(all_data.Date<=max_date)].reset_index(drop=True)\n",
    "\n",
    "    for col in ['EUR/USD_T','EUR/GBP_T','GBP/USD_T','XAU/USD_T']:\n",
    "        data[col] = data[col.split('_')[0]+'_R']\n",
    "\n",
    "    for col in ['EUR/USD_R','EUR/GBP_R','GBP/USD_R','XAU/USD_R']:\n",
    "        data[col] = data[col].shift(1)\n",
    "        \n",
    "    data = data.dropna(subset=['EUR/USD_R','EUR/GBP_R','GBP/USD_R','XAU/USD_R'])\n",
    "    data = data.sort_values(by=[\"Date\"]).reset_index(drop=True)\n",
    "    \n",
    "    # Defining the Length of Test Portion\n",
    "    data[\"Portion\"] = \"Train\"\n",
    "    data.loc[(data.Date>=min_date)&(data.Date<=max_date),\"Portion\"] = \"Test\"\n",
    "    first_test_index = data.loc[data['Portion']==\"Test\"].index[0]\n",
    "    data.loc[first_test_index-(context_length+max(lags)-1):,[\"Portion\"]] = \"Test\"\n",
    "    \n",
    "    # Transforming the Date Column as per Informer Requirements\n",
    "    data[\"Orig_Date\"] = data[\"Date\"]\n",
    "    data[\"Date\"] = pd.date_range(data[\"Date\"].iloc[0],data[\"Date\"].iloc[0]+pd.DateOffset(hours=len(data)-1),freq='1H')\n",
    "    data = data[[\"Orig_Date\",\"Date\",\"GBP/USD_T\",\"EUR/GBP_T\",\"EUR/USD_T\",\"XAU/USD_T\",\"GBP/USD\",\"EUR/USD\",\"EUR/GBP\",\"XAU/USD\",\"Portion\"]]\n",
    "    data = data.rename(columns={\"Date\":\"start\"})\n",
    "    \n",
    "    # Train, Validation and Test Split\n",
    "    train_data,valid_data,test_data = data_split(df=data,train_size=0.95)\n",
    "\n",
    "    # Making Copies of Dataframes for later use\n",
    "    orig_valid = valid_data.copy()\n",
    "    orig_valid = orig_valid.rename(columns={\"Orig_Date\":\"Date\"})\n",
    "    orig_valid = orig_valid.drop('start',axis=1)\n",
    "\n",
    "    orig_test = test_data.copy()\n",
    "    orig_test = orig_test.rename(columns={\"Orig_Date\":\"Date\"})\n",
    "    orig_test = orig_test.drop('start',axis=1)\n",
    "    \n",
    "    train_data = train_data.drop([\"Orig_Date\",\"GBP/USD\",\"EUR/USD\",\"EUR/GBP\",\"XAU/USD\"],axis=1).reset_index(drop=True)\n",
    "    valid_data = valid_data.drop([\"Orig_Date\",\"GBP/USD\",\"EUR/USD\",\"EUR/GBP\",\"XAU/USD\"],axis=1).reset_index(drop=True)\n",
    "    test_data = test_data.drop([\"Orig_Date\",\"GBP/USD\",\"EUR/USD\",\"EUR/GBP\",\"XAU/USD\"],axis=1).reset_index(drop=True)\n",
    "    \n",
    "    # Defining the Informer Model\n",
    "    config = InformerConfig(input_size = num_of_variates,\n",
    "                            prediction_length = prediction_length,\n",
    "                            context_length = context_length,\n",
    "                            lags_sequence = lags,\n",
    "                            num_time_features = num_time_features,   \n",
    "                            dropout=dropout,\n",
    "                            encoder_layers=encoder_layers,\n",
    "                            decoder_layers=decoder_layers,\n",
    "                            d_model=d_model,\n",
    "                            attention_type=attention_type)\n",
    "    model = InformerForPrediction(config)\n",
    "    \n",
    "    # Transforming Train Data using DataLoader \n",
    "    train_dataloader = create_dataloader(train_data,num_of_variates,freq,config,'train')\n",
    "\n",
    "    # Model Training\n",
    "    accelerator = Accelerator()\n",
    "    device = accelerator.device\n",
    "    optimizer = AdamW(model.parameters(),lr=learning_rate,betas=(0.9,0.95),weight_decay=1e-1)\n",
    "    model = train_model(train_dataloader,model,device,optimizer,accelerator,epochs,config)\n",
    "\n",
    "    # Generating Predictions on the Validation Set\n",
    "    valid_forecasts = generate_forecasts(valid_data,model,num_of_variates,context_length,lags,freq,config,device)\n",
    "    valid_actual = orig_valid.iloc[(context_length+max(lags)-1):].reset_index(drop=True)\n",
    "    valid_pred = pd.DataFrame(data=valid_forecasts,columns=['GBP/USD_P','EUR/GBP_P','EUR/USD_P','XAU/USD_P'])\n",
    "    valid_comb = pd.concat([valid_actual,valid_pred],axis=1)\n",
    "\n",
    "    # Generating Predictions on the Test Set\n",
    "    test_forecasts = generate_forecasts(test_data,model,num_of_variates,context_length,lags,freq,config,device)\n",
    "    test_actual = orig_test.iloc[(context_length+max(lags)-1):].reset_index(drop=True)\n",
    "    test_pred = pd.DataFrame(data=test_forecasts,columns=['GBP/USD_P','EUR/GBP_P','EUR/USD_P','XAU/USD_P'])\n",
    "    test_comb = pd.concat([test_actual,test_pred],axis=1)\n",
    "    test_comb[\"Test_Portion\"] = p\n",
    "    \n",
    "    # Mean Squared Error Calculation\n",
    "    valid_mses = []\n",
    "    test_mses = []\n",
    "    for col in ['GBP/USD','EUR/GBP','EUR/USD','XAU/USD']:\n",
    "\n",
    "        test_comb[col+'_PP'] = (test_comb[col+'_P']+1) * test_comb[col]\n",
    "        test_comb[col+'_PP'] = test_comb[col+'_PP'].shift(1)\n",
    "\n",
    "        valid_mses.append(mean_squared_error(valid_comb[col+'_T'],valid_comb[col+'_P']))\n",
    "        test_mses.append(mean_squared_error(test_comb[col+'_T'],test_comb[col+'_P']))\n",
    "\n",
    "    # Saving Test Results\n",
    "    final_results = pd.concat([final_results,test_comb])\n",
    "\n",
    "    print('#' * 25)\n",
    "    print('### Test Portion', p+1)\n",
    "    print(\"### Average Valid MSE = \",np.mean(valid_mses))\n",
    "    print(\"### Average Test MSE = \",np.mean(test_mses))\n",
    "    print('#' * 25)\n",
    "\n",
    "final_results.to_csv(results_path+\"Test_Results.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
