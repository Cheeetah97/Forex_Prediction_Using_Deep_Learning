{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow_probability as tfp\n",
    "from Functions import *\n",
    "tf.keras.utils.set_random_seed(0)\n",
    "\n",
    "data_path = \"M:/Dissertation/Data/\"\n",
    "test_data_path = \"M:/Dissertation/Return_Prediction/Deep_Learning/Results/\"\n",
    "results_path = \"M:/Dissertation/Price_Prediction/Results/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Extracting Test Portions**  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Portion</th>\n",
       "      <th>MinDate</th>\n",
       "      <th>MaxDate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2016-06-20 23:00:00</td>\n",
       "      <td>2016-06-24 01:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2017-11-06 08:00:00</td>\n",
       "      <td>2017-11-09 10:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2017-09-07 10:00:00</td>\n",
       "      <td>2017-09-12 13:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2016-07-22 08:00:00</td>\n",
       "      <td>2016-07-27 11:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2016-10-31 06:00:00</td>\n",
       "      <td>2016-11-03 08:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Portion             MinDate             MaxDate\n",
       "0        0 2016-06-20 23:00:00 2016-06-24 01:00:00\n",
       "1        1 2017-11-06 08:00:00 2017-11-09 10:00:00\n",
       "2        2 2017-09-07 10:00:00 2017-09-12 13:00:00\n",
       "3        3 2016-07-22 08:00:00 2016-07-27 11:00:00\n",
       "4        4 2016-10-31 06:00:00 2016-11-03 08:00:00"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_portions = pd.read_csv(test_data_path+\"Test_Results.csv\")\n",
    "test_portions[\"Date\"] = pd.to_datetime(test_portions[\"Date\"],format=\"%Y-%m-%d %H:00:00\")\n",
    "test_portions = pd.concat([test_portions.groupby(by=[\"Portion\"],as_index=False).agg({\"Date\":\"min\"}),\n",
    "                           test_portions.groupby(by=[\"Portion\"]).agg({\"Date\":\"max\"})],ignore_index=True,axis=1).rename(columns={0:\"Portion\",1:\"MinDate\",2:\"MaxDate\"})\n",
    "test_portions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the Data\n",
    "all_data = pd.read_csv(data_path+\"Forex_Data.csv\")\n",
    "all_data[\"Date\"] = pd.to_datetime(all_data[\"Date\"],format=\"%Y-%m-%d %H:00:00\")\n",
    "all_data[\"Test_Portion\"] = 0\n",
    "\n",
    "# Iterating over the Test Portions\n",
    "final_results = pd.DataFrame()\n",
    "for p in list(test_portions.Portion.unique()):\n",
    "\n",
    "    min_date = test_portions.loc[test_portions.Portion==p,\"MinDate\"].iloc[0]\n",
    "    max_date = test_portions.loc[test_portions.Portion==p,\"MaxDate\"].iloc[0]\n",
    "\n",
    "    all_data.loc[(all_data.Date>=min_date)&(all_data.Date<=max_date),\"Test_Portion\"] = 1\n",
    "\n",
    "all_data.to_csv(\"TEST_PORTIONS.csv\",index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **RNN Params**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Related\n",
    "window_size = 12\n",
    "train_size = 0.95\n",
    "\n",
    "# Model Related\n",
    "n_neurons = 164\n",
    "learning_rate = 0.005\n",
    "batch_size = 48\n",
    "epochs = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training the Model and Generating Predictions for Each Test Portion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#########################\n",
      "### Portion 1\n",
      "### Avg Validation Accuracy Score: 0.5558312655086848\n",
      "### Avg Test Accuracy Score: 0.5902777777777778\n",
      "#########################\n",
      "#########################\n",
      "### Portion 2\n",
      "### Avg Validation Accuracy Score: 0.576302729528536\n",
      "### Avg Test Accuracy Score: 0.5416666666666666\n",
      "#########################\n",
      "#########################\n",
      "### Portion 3\n",
      "### Avg Validation Accuracy Score: 0.5517990074441688\n",
      "### Avg Test Accuracy Score: 0.5729166666666667\n",
      "#########################\n",
      "#########################\n",
      "### Portion 4\n",
      "### Avg Validation Accuracy Score: 0.5603341584158416\n",
      "### Avg Test Accuracy Score: 0.5381944444444444\n",
      "#########################\n",
      "#########################\n",
      "### Portion 5\n",
      "### Avg Validation Accuracy Score: 0.5490074441687345\n",
      "### Avg Test Accuracy Score: 0.5277777777777778\n",
      "#########################\n"
     ]
    }
   ],
   "source": [
    "# Reading the Data\n",
    "all_data = pd.read_csv(data_path+\"Forex_Data.csv\")\n",
    "all_data[\"Date\"] = pd.to_datetime(all_data[\"Date\"],format=\"%Y-%m-%d %H:00:00\")\n",
    "\n",
    "# Iterating over the Test Portions\n",
    "final_results = pd.DataFrame()\n",
    "for p in list(test_portions.Portion.unique()):\n",
    "\n",
    "    # Slicing the Data as per the Min and Max Dates of Each Test Portion\n",
    "    min_date = test_portions.loc[test_portions.Portion==p,\"MinDate\"].iloc[0]\n",
    "    max_date = test_portions.loc[test_portions.Portion==p,\"MaxDate\"].iloc[0]\n",
    "    \n",
    "    data = all_data.loc[(all_data.Date>=min_date-pd.DateOffset(years=3))&(all_data.Date<=max_date)].reset_index(drop=True)\n",
    "\n",
    "    # Making Return as Feature\n",
    "    for col in ['EUR/USD_R','EUR/GBP_R','GBP/USD_R','XAU/USD_R']:\n",
    "        data[col] = data[col].shift(1)\n",
    "    data = data.dropna(subset=['EUR/USD_R','EUR/GBP_R','GBP/USD_R','XAU/USD_R'])\n",
    "    data = data.sort_values(by=[\"Date\"]).reset_index(drop=True)\n",
    "    orig_data = data.copy()\n",
    "\n",
    "    # Setting Test Values to NaN\n",
    "    data.loc[(data.Date>=min_date)&(data.Date<=max_date),[\"GBP/USD_T\",\"EUR/GBP_T\",\"EUR/USD_T\",\"XAU/USD_T\"]] = np.nan\n",
    "\n",
    "    # Features\n",
    "    x = data[[\"GBP/USD\",\"EUR/USD\",\"EUR/GBP\",\"XAU/USD\",\"GBP/USD_R\",\"EUR/USD_R\",\"EUR/GBP_R\",\"XAU/USD_R\"]]\n",
    "\n",
    "    # Targets\n",
    "    y_1 = data[[\"GBP/USD_T\"]]\n",
    "    y_2 = data[[\"EUR/USD_T\"]]\n",
    "    y_3 = data[[\"EUR/GBP_T\"]]\n",
    "    y_4 = data[[\"XAU/USD_T\"]]\n",
    "\n",
    "    # Generating Data for LSTM\n",
    "    train_x,valid_x,test_x,orig_train_x,orig_valid_x,orig_test_x,train_y1,valid_y1 = rnn_data_prep(x,y_1,window_size,train_size,TSScaler(),orig_data,'1')\n",
    "    train_x,valid_x,test_x,orig_train_x,orig_valid_x,orig_test_x,train_y2,valid_y2 = rnn_data_prep(x,y_2,window_size,train_size,TSScaler(),orig_data,'2')\n",
    "    train_x,valid_x,test_x,orig_train_x,orig_valid_x,orig_test_x,train_y3,valid_y3 = rnn_data_prep(x,y_3,window_size,train_size,TSScaler(),orig_data,'3')\n",
    "    train_x,valid_x,test_x,orig_train_x,orig_valid_x,orig_test_x,train_y4,valid_y4 = rnn_data_prep(x,y_4,window_size,train_size,TSScaler(),orig_data,'4')\n",
    "\n",
    "    # Defining the Model\n",
    "    model = RNN(window_size,train_x.shape[2],n_neurons,'tanh',weighted_mse,tf.keras.optimizers.Adam(learning_rate=learning_rate))\n",
    "\n",
    "    # Early stopping and Model Checkpoint\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor='val_loss',mode='min',verbose=0,patience=30)\n",
    "    mc = tf.keras.callbacks.ModelCheckpoint('lstm.h5',monitor='val_loss',mode='min',verbose=0,save_best_only=True)\n",
    "\n",
    "    # Training the Model\n",
    "    model.fit([train_x],[train_y1,train_y2,train_y3,train_y4],validation_data=([valid_x],[valid_y1,valid_y2,valid_y3,valid_y4]),batch_size=batch_size,epochs=epochs,callbacks=[es,mc],use_multiprocessing=True,verbose=0)\n",
    "    tf.keras.backend.clear_session()\n",
    "\n",
    "    # Loading Scaler Objects\n",
    "    scaler_objs = []\n",
    "    with open('scaler_1.pkl','rb') as file:\n",
    "        y_scaler_1 = pickle.load(file)\n",
    "        scaler_objs.append(y_scaler_1)\n",
    "    with open('scaler_2.pkl','rb') as file:\n",
    "        y_scaler_2 = pickle.load(file)\n",
    "        scaler_objs.append(y_scaler_2)\n",
    "    with open('scaler_3.pkl','rb') as file:\n",
    "        y_scaler_3 = pickle.load(file)\n",
    "        scaler_objs.append(y_scaler_3)\n",
    "    with open('scaler_4.pkl','rb') as file:\n",
    "        y_scaler_4 = pickle.load(file)\n",
    "        scaler_objs.append(y_scaler_4)\n",
    "\n",
    "    # Loading Model\n",
    "    model = tf.keras.models.load_model('lstm.h5',custom_objects={'attention':attention,'weighted_mse':weighted_mse})\n",
    "\n",
    "    # Predictions on Validation Set\n",
    "    valid_pred = model.predict([valid_x],verbose=0)\n",
    "    valid_pred = compile_predictions(valid_pred,len(valid_pred),scaler_objs,[\"GBP/USD_P\",\"EUR/USD_P\",\"EUR/GBP_P\",\"XAU/USD_P\"])\n",
    "    orig_valid_x = pd.concat([orig_valid_x,valid_pred],axis=1).reset_index(drop=True)\n",
    "    orig_valid_x = orig_valid_x[['Date','EUR/USD_P','EUR/GBP_P','GBP/USD_P','XAU/USD_P','EUR/USD_T','EUR/GBP_T','GBP/USD_T','XAU/USD_T']]\n",
    "    \n",
    "    valid_accs = []\n",
    "    for col in [\"GBP/USD\",\"EUR/USD\",\"EUR/GBP\",\"XAU/USD\"]:\n",
    "        orig_valid_x[col+\"_R\"] = (orig_valid_x[col+\"_T\"].shift(-1)/orig_valid_x[col+\"_T\"])-1\n",
    "        orig_valid_x[col+\"_PR\"] = (orig_valid_x[col+\"_P\"].shift(-1)/orig_valid_x[col+\"_P\"])-1\n",
    "        \n",
    "        orig_valid_x[\"T1\"] = orig_valid_x[col+\"_T\"].shift(-1)\n",
    "        orig_valid_x[\"P1\"] = orig_valid_x[col+\"_P\"].shift(-1)\n",
    "\n",
    "        orig_valid_x[\"T_Inc\"] = 0\n",
    "        orig_valid_x.loc[orig_valid_x[\"T1\"] > orig_valid_x[col+\"_T\"],\"T_Inc\"] = 1\n",
    "\n",
    "        orig_valid_x[\"P_Inc\"] = 0\n",
    "        orig_valid_x.loc[orig_valid_x[\"P1\"] > orig_valid_x[col+\"_P\"],\"P_Inc\"] = 1\n",
    "\n",
    "        valid_accs.append(accuracy_score(orig_valid_x[\"T_Inc\"],orig_valid_x[\"P_Inc\"]))\n",
    "    \n",
    "    # Predictions on Test Set\n",
    "    test_pred = model.predict([test_x],verbose=0)\n",
    "    test_pred = compile_predictions(test_pred,len(test_pred),scaler_objs,[\"GBP/USD_P\",\"EUR/USD_P\",\"EUR/GBP_P\",\"XAU/USD_P\"])\n",
    "    orig_test_x = pd.concat([orig_test_x,test_pred],axis=1).reset_index(drop=True)\n",
    "    orig_test_x = orig_test_x[['Date','EUR/USD_P','EUR/GBP_P','GBP/USD_P','XAU/USD_P','EUR/USD_T','EUR/GBP_T','GBP/USD_T','XAU/USD_T']]\n",
    "    orig_test_x[\"Portion\"] = p\n",
    "\n",
    "    test_accs = []\n",
    "    for col in [\"GBP/USD\",\"EUR/USD\",\"EUR/GBP\",\"XAU/USD\"]:\n",
    "\n",
    "        orig_test_x[col+\"_R\"] = (orig_test_x[col+\"_T\"].shift(-1)/orig_test_x[col+\"_T\"])-1\n",
    "        orig_test_x[col+\"_PR\"] = (orig_test_x[col+\"_P\"].shift(-1)/orig_test_x[col+\"_P\"])-1\n",
    "\n",
    "        orig_test_x[\"T1\"] = orig_test_x[col+\"_T\"].shift(-1)\n",
    "        orig_test_x[\"P1\"] = orig_test_x[col+\"_P\"].shift(-1)\n",
    "\n",
    "        orig_test_x[\"T_Inc\"] = 0\n",
    "        orig_test_x.loc[orig_test_x[\"T1\"] > orig_test_x[col+\"_T\"],\"T_Inc\"] = 1\n",
    "\n",
    "        orig_test_x[\"P_Inc\"] = 0\n",
    "        orig_test_x.loc[orig_test_x[\"P1\"] > orig_test_x[col+\"_P\"],\"P_Inc\"] = 1\n",
    "\n",
    "        test_accs.append(accuracy_score(orig_test_x[\"T_Inc\"],orig_test_x[\"P_Inc\"]))\n",
    "\n",
    "    orig_test_x = orig_test_x.drop([\"T1\",\"P1\",\"T_Inc\",\"P_Inc\"],axis=1)\n",
    "    final_results = pd.concat([final_results,orig_test_x])\n",
    "\n",
    "    print('#' * 25)\n",
    "    print('### Portion', p + 1)\n",
    "    print('### Avg Validation Accuracy Score:', np.mean(valid_accs))\n",
    "    print('### Avg Test Accuracy Score:', np.mean(test_accs))\n",
    "    print('#' * 25)\n",
    "\n",
    "# Saving the Results\n",
    "final_results.to_csv(results_path+\"Test_Results.csv\",index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
